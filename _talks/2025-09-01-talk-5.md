---
title: "Build-LLAMA2: Mini Llama2 Transformer Implementation"
collection: talks
type: "Course Project"
permalink: /talks/2025-09-01-talk-5
venue: "Carnegie Mellon University"
date: Aug. 2025 - Sep. 2025
location: "Pittsburgh, USA"
---

Deep Learning, Natural Language Processing

Project Member

•	Implemented core components of the Llama2 transformer architecture from scratch, including Grouped Query Attention (GQA), feed-forward networks, Pre-LayerNorm, Rotary Position Embeddings (RoPE), and AdamW optimizer.

•	Integrated parameter-efficient fine-tuning techniques via LoRA and WiSE-FT for enhanced model adaptability.

•	Applied the model to tasks such as text continuation, zero-shot classification, and downstream fine-tuning, demonstrating strong performance on various NLP benchmarks.


